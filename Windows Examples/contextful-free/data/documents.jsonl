{"doc": "---\ntitle: pathway.io.sqlite package\nsidebar: 'API'\nnavigation: false\n---\n# pathway.io.sqlite package\nFunctions\npw.io.sqlite.read(path, table_name, schema, *, autocommit_duration_ms=1500, debug_data=None)\nReads a table from a rowid table in SQLite database.\n* Parameters\n    * path (`PathLike` | `str`) \u2013 Path to the database file.\n    * table_name (`str`) \u2013 Name of the table in the database to be read.\n    * schema (`type`\\[`Schema`\\]) \u2013 Schema of the resulting table.\n    * autocommit_duration_ms (`Optional`\\[`int`\\]) \u2013 The maximum time between two commits. Every\n        autocommit_duration_ms milliseconds, the updates received by the connector are\n        committed and pushed into Pathway\u2019s computation graph.\n* Returns\n    *Table* \u2013 The table read.\n"}
{"doc": "Notes\nThe CSV files should follow a standard CSV settings: the separator is \u2018,\u2019, the\nquotechar is \u2018\u201d\u2019, and there is no escape.\npw.demo.replay_csv_with_time(path, *, schema, time_column, unit='s', autocommit_ms=100, speedup=1)\nReplay a static CSV files as a data stream while respecting the time between updated based on a timestamp columns.\nThe timestamps in the file should be ordered positive integers.\n* Parameters\n    * path (`str`) \u2013 Path to the file to stream.\n    * schema (`type`\\[`Schema`\\]) \u2013 Schema of the resulting table.\n    * time_column (`str`) \u2013 Column containing the timestamps.\n    * unit (`str`) \u2013 Unit of the timestamps. Only \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019, and \u2018ns\u2019 are supported. Defaults to \u2018s\u2019.\n    * autocommit_duration_ms \u2013 the maximum time between two commits. Every\n        autocommit_duration_ms milliseconds, the updates received by the connector are\n        committed and pushed into Pathway\u2019s computation graph.\n    * speedup (`float`) \u2013 Produce stream speedup times faster than it would result from the time column.\n* Returns\n    *Table* \u2013 The table read.\n"}
{"doc": "---\ntitle: Demo API\nsidebar: 'API'\nnavigation: true\n---\n# Demo API\nThe demo module allows you to create custom data streams from scratch or by utilizing a CSV file.\nThis feature empowers you to effectively test and debug your Pathway implementation using realtime data.\nPathway demo module\nTypical use:\n```python\nclass InputSchema(pw.Schema):\n   name: str\n   age: int\npw.demo.replay_csv(\"./input_stream.csv\", schema=InputSchema)\n```\n::\nResult\n```\n, 'age': }>\n```\n::\n::\nFunctions\npw.demo.generate_custom_stream(value_generators, *, schema, nb_rows=None, autocommit_duration_ms=1000, input_rate=1.0, persistent_id=None)\nGenerates a data stream.\nThe generator creates a table and periodically streams rows.\nIf a `nb_rows` value is provided, there are `nb_rows` row generated in total,\nelse the generator streams indefinitely.\nThe rows are generated iteratively and have an associated index x, starting from 0.\nThe values of each column are generated by their associated function in `value_generators`.\n* Parameters\n    * value_generators (`dict`\\[`str`, `Any`\\]) \u2013 Dictionary mapping column names to functions that generate values for each column.\n    * schema (`type`\\[`Schema`\\]) \u2013 Schema of the resulting table.\n    * nb_rows (`Optional`\\[`int`\\]) \u2013 The number of rows to generate. Defaults to None. If set to None, the generator\n        generates streams indefinitely.\n    * types \u2013 Dictionary containing the mapping between the columns and the data types (`pw.Type`) of the values of those columns. This parameter is optional, and if not provided the default type is `pw.Type.ANY`.\n    * autocommit_duration_ms (`int`) \u2013 the maximum time between two commits. Every\n        autocommit_duration_ms milliseconds, the updates received by the connector are\n        committed and pushed into Pathway\u2019s computation graph.\n    * input_rate (`float`) \u2013 The rate at which rows are generated per second. Defaults to 1.0.\n* Returns\n    *Table* \u2013 The generated table.\nExample:\n"}
{"doc": "---\ntitle: Demo API\nsidebar: 'API'\nnavigation: true\n---\n# Demo API\nThe demo module allows you to create custom data streams from scratch or by utilizing a CSV file.\nThis feature empowers you to effectively test and debug your Pathway implementation using realtime data.\nPathway demo module\nTypical use:\n```python\nvalue_functions = {\n    'number': lambda x: x + 1,\n    'name': lambda x: f'Person {x}',\n    'age': lambda x: 20 + x,\n}\nclass InputSchema(pw.Schema):\n     number: int\n     name: str\n     age: int\npw.demo.generate_custom_stream(value_functions, schema=InputSchema, nb_rows=10)\n```\n::\nResult\n```\n, 'name': , 'age': }>\n```\n::\n::\nIn the above example, a data stream is generated with 10 rows, where each row has columns         \u2018number\u2019, \u2018name\u2019, and \u2018age\u2019.\nThe \u2018number\u2019 column contains values incremented by 1 from 1 to 10, the \u2018name\u2019 column contains \u2018Person\u2019\nfollowed by the respective row index, and the \u2018age\u2019 column contains values starting from 20 incremented by\nthe row index.\npw.demo.noisy_linear_stream(nb_rows=10, input_rate=1.0)\nGenerates an artificial data stream for the linear regression tutorial.\n* Parameters\n    * nb_rows (*int, optional*) \u2013 The number of rows to generate in the data stream. Defaults to 10.\n    * input_rate (*float, optional*) \u2013 The rate at which rows are generated per second. Defaults to 1.0.\n* Returns\n    *pw.Table* \u2013 A table containing the generated data stream.\nExample:\n```python\ntable = pw.demo.noisy_linear_stream(nb_rows=100, input_rate=2.0)\n```\nIn the above example, an artificial data stream is generated with 100 rows. Each row has two columns, \u2018x\u2019 and \u2018y\u2019.\nThe \u2018x\u2019 values range from 0 to 99, and the \u2018y\u2019 values are equal to \u2018x\u2019 plus some random noise.\npw.demo.range_stream(nb_rows=30, offset=0, input_rate=1.0)\nGenerates a simple artificial data stream, used to compute the sum in our examples.\n* Parameters\n    * nb_rows (*int, optional*) \u2013 The number of rows to generate in the data stream. Defaults to 30.\n    * offset (*int, optional*) \u2013 The offset value added to the generated \u2018value\u2019 column. Defaults to 0.\n    * input_rate (*float, optional*) \u2013 The rate at which rows are generated per second. Defaults to 1.0.\n* Returns\n    *pw.Table* \u2013 a table containing the generated data stream.\nExample:\n```python\ntable = pw.demo.range_stream(nb_rows=50, offset=10, input_rate=2.5)\n```\nIn the above example, an artificial data stream is generated with a single column \u2018value\u2019 and 50 rows.\nThe \u2018value\u2019 column contains values ranging from \u2018offset\u2019 (10 in this case) to \u2018nb_rows\u2019 + \u2018offset\u2019 (60).\npw.demo.replay_csv(path, *, schema, input_rate=1.0)\nReplay a static CSV files as a data stream.\n* Parameters\n    * path (`str` | `PathLike`) \u2013 Path to the file to stream.\n    * schema (`type`\\[`Schema`\\]) \u2013 Schema of the resulting table.\n    * autocommit_duration_ms \u2013 the maximum time between two commits. Every\n        autocommit_duration_ms milliseconds, the updates received by the connector are\n        committed and pushed into Pathway\u2019s computation graph.\n    * input_rate (*float, optional*) \u2013 The rate at which rows are read per second. Defaults to 1.0.\n* Returns\n    *Table* \u2013 The table read.\n"}
{"doc": "---\ntitle: pathway.stdlib.statistical package\nsidebar: 'API'\nnavigation: false\n---\n# pathway.stdlib.statistical package\nFunctions\npw.statistical.interpolate(self, timestamp, *values, mode=InterpolateMode.LINEAR)\nInterpolates missing values in a column using the previous and next values based on a timestamps column.\n* Parameters\n    * timestamp (*ColumnReference*) \u2013 Reference to the column containing timestamps.\n    * \\*values (*ColumnReference*) \u2013 References to the columns containing values to be interpolated.\n    * mode (*InterpolateMode, optional*) \u2013 The interpolation mode. Currently,            only InterpolateMode.LINEAR is supported. Default is InterpolateMode.LINEAR.\n* Returns\n    *Table* \u2013 A new table with the interpolated values.\n* Raises\n    ValueError \u2013 If the columns are not ColumnReference or if the interpolation mode is not supported.\nNOTE: * The interpolation is performed based on linear interpolation between the previous and next values.\n* If a value is missing at the beginning or end of the column, no interpolation is performed.\nExample:\nCode\n```python\nimport pathway as pw\ntable = pw.debug.table_from_markdown('''\ntimestamp | values_a | values_b\n1         | 1        | 10\n2         |          |\n3         | 3        |\n4         |          |\n5         |          |\n6         | 6        | 60\n''')\ntable = table.interpolate(pw.this.timestamp, pw.this.values_a, pw.this.values_b)\npw.debug.compute_and_print(table, include_id=False)\n```\n::\nResult\n```\ntimestamp | values_a | values_b\n1         | 1        | 10\n2         | 2.0      | 20.0\n3         | 3        | 30.0\n4         | 4.0      | 40.0\n5         | 5.0      | 50.0\n6         | 6        | 60\n```\n::\n::\n"}
{"doc": "pathway.stdlib.graphs.louvain_communities.impl module\nFunctions\npw.graphs.louvain_communities.impl.exact_modularity(G, C, round_digits=16)\nThis function computes modularity of a given weighted graph G with\nrespect to clustering C.\nThis implementation is meant to be used for testing / development,\nas computing exact value requires us to know the exact sum of the edge weights,\nwhich creates long dependency chains, and may be slow.\nThis implementation rounds the modularity to round_digits decimal places\n(default is 16), for result res it returns round(res, ndigits = round_digits)\n"}
{"doc": "pathway.stdlib.ml.classifiers.test_lsh module\npw.ml.classifiers.test_lsh.test_bucketer_cosine()\nVerifies that L buckets were indeed created\npw.ml.classifiers.test_lsh.test_bucketer_euclidean()\nVerifies that L buckets were indeed created\npw.ml.classifiers.test_lsh.test_lsh()\nVerifies that close points are mapped together and distant ones - apart.\npw.ml.classifiers.test_lsh.test_lsh_bucketing()\nVerifies that bucketing is properly indexed.\n"}
{"doc": "---\ntitle: pathway.stdlib.ml.classifiers package\nsidebar: 'API'\nnavigation: false\n---\n# pathway.stdlib.ml.classifiers package\nFunctions\npw.ml.classifiers.knn_lsh_classifier_train(data, L, type='euclidean', kwargs)\nBuild the LSH index over data.\nL the number of repetitions of the LSH scheme.\nReturns a LSH projector of type (queries: Table, k:Any) -> Table\npw.ml.classifiers.knn_lsh_classify(knn_model, data_labels, queries, k)\nClassify the queries.\nUse the knn_model to extract the k closest datapoints.\nThe queries are then labeled using a majority vote between the labels\nof the retrieved datapoints, using the labels provided in data_labels.\npw.ml.classifiers.knn_lsh_euclidean_classifier_train(data, d, M, L, A)\nBuild the LSH index over data using the Euclidean distances.\nd is the dimension of the data, L the number of repetition of the LSH scheme,\nM and A are specific to LSH with Euclidean distance, M is the number of random projections\ndone to create each bucket and A is the width of each bucket on each projection.\npw.ml.classifiers.knn_lsh_generic_classifier_train(data, lsh_projection, distance_function, L)\nBuild the LSH index over data using the a generic lsh_projector and its associated distance.\nL the number of repetitions of the LSH scheme.\nReturns a LSH projector of type (queries: Table, k:Any) -> Table\npw.ml.classifiers.knn_lsh_train(data, L, type='euclidean', kwargs)\nBuild the LSH index over data.\nL the number of repetitions of the LSH scheme.\nReturns a LSH projector of type (queries: Table, k:Any) -> Table\n"}
{"doc": "---\ntitle: pathway.io.fs package\nsidebar: 'API'\nnavigation: false\n---\n# pathway.io.fs package\nFunctions\npw.io.fs.read(path, format, *, schema=None, mode='streaming', csv_settings=None, json_field_paths=None, object_pattern='*', with_metadata=False, persistent_id=None, autocommit_duration_ms=1500, debug_data=None, value_columns=None, primary_key=None, types=None, default_values=None)\nReads a table from one or several files with the specified format.\nIn case the folder is passed to the engine, the order in which files from the\ndirectory are processed is determined according to the modification time of files\nwithin this folder: they will be processed by ascending order of the modification time.\nIn case the format is \u201cplaintext\u201d, the table will consist of a single column\n`data` with each cell containing a single line from the file.\n* Parameters\n    * path (`str` | `PathLike`) \u2013 Path to the file or to the folder with files.\n    * format (`str`) \u2013 Format of data to be read. Currently \u201ccsv\u201d, \u201cjson\u201d, \u201cplaintext\u201d, \u201cplaintext_by_file\u201d and \u201cbinary\u201d formats are supported. The difference between \u201cplaintext\u201d and \u201cplaintext_by_file\u201d is how the input is tokenized: if the \u201cplaintext\u201d option is chosen, it\u2019s split by the newlines. Otherwise, the files are split in full and one row will correspond to one file. In case the \u201cbinary\u201d format is specified, the data is read as raw bytes without UTF-8 parsing.\n    * schema (`Optional`\\`type`\\[[`Schema`\\]\\]) \u2013 Schema of the resulting table.\n    * mode (`str`) \u2013 denotes how the engine polls the new data from the source. Currently \u201cstreaming\u201d, \u201cstatic\u201d, and \u201cstreaming_with_deletions\u201d are supported. If set to \u201cstreaming\u201d the engine will wait for the new input files in the directory. On the other hand, \u201cstreaming_with_deletions\u201d mode also tracks file deletions and modifications and reflects them in the state. For example, if a file was deleted, \u201cstreaming_with_deletions\u201dmode will also remove rows obtained by reading this file from the table. Finally, the \u201cstatic\u201d mode will only consider the available data and ingest all of it in one commit. The default value is \u201cstreaming\u201d.\n    * csv_settings (`Optional`\\[`CsvParserSettings`\\]) \u2013 Settings for the CSV parser. This parameter is used only in case\n        the specified format is \u201ccsv\u201d.\n    * json_field_paths (`Optional`\\[`dict`\\[`str`, `str`\\]\\]) \u2013 If the format is \u201cjson\u201d, this field allows to map field names\n        into path in the read json object. For the field which require such mapping,\n        it should be given in the format `: `,\n        where the path to be mapped needs to be a\n        JSON Pointer (RFC 6901).\n    * object_pattern (`str`) \u2013 Unix shell style pattern for filtering only certain files in the directory. Ignored in case a path to a single file is specified.\n    * with_metadata (`bool`) \u2013 When set to true, the connector will add an additional column named `_metadata` to the table. This column will be a JSON field that will contain two optional fields - `created_at` and `modified_at`. These fields will have integral UNIX timestamps for the creation and modification time respectively. Additionally, the column will also have an optional field named `owner` that will contain the name of the file owner (applicable only for Un). Finally, the column will also contain a field named `path` that will show the full path to the file from where a row was filled.\n    * persistent_id (`Optional`\\[`str`\\]) \u2013 (unstable) An identifier, under which the state of the table\n        will be persisted or `None`, if there is no need to persist the state of this table.\n        When a program restarts, it restores the state for all input tables according to what\n        was saved for their `persistent_id`. This way it\u2019s possible to configure the start of\n        computations from the moment they were terminated last time.\n    * debug_data (`Any`) \u2013 Static data replacing original one when debug mode is active.\n    * value_columns (`Optional`\\[`list`\\[`str`\\]\\]) \u2013 Names of the columns to be extracted from the files. \\[will be deprecated soon\\]\n    * primary_key (`Optional`\\[`list`\\[`str`\\]\\]) \u2013 In case the table should have a primary key generated according to\n        a subset of its columns, the set of columns should be specified in this field.\n        Otherwise, the primary key will be generated randomly. \\[will be deprecated soon\\]\n    * types (`Optional`\\[`dict`\\[`str`, `PathwayType`\\]\\]) \u2013 Dictionary containing the mapping between the columns and the data\n        types (`pw.Type`) of the values of those columns. This parameter is optional, and if not\n        provided the default type is `pw.Type.ANY`. Supported in \u201ccsv\u201d and \u201cjson\u201d formats.\n        \\[will be deprecated soon\\]\n    * default_values (`Optional`\\[`dict`\\[`str`, `Any`\\]\\]) \u2013 dictionary containing default values for columns replacing\n        blank entriest value of the column must be specified explicitly,\n        otherwise there will be no default value. \\[will be deprecated soon\\]\n* Returns\n    *Table* \u2013 The table read.\nExample:\nConsider you want to read a dataset, stored in the filesystem in a standard CSV\nformat. The dataset contains data about pets and their owners.\nFor the sake of demonstration, you can prepare a small dataset by creating a CSV file\nvia a unix command line tool:\n```bash\nprintf \"id,owner,pet\\n1,Alice,dog\\n2,Bob,dog\\n3,Alice,cat\\n4,Bob,dog\" > dataset.csv\n```\nIn order to read it into Pathway\u2019s table, you can first do the import and then\nuse the `pw.io.fs.read` method:\n```python\nimport pathway as pw\nclass InputSchema(pw.Schema):\n  owner: str\n  pet: str\nt = pw.io.fs.read(\"dataset.csv\", format=\"csv\", schema=InputSchema)\n```\nThen, you can output the table in order to check the correctness of the read:\nCode\n```python\npw.debug.compute_and_print(t, include_id=False)  \n```\n::\nResult\n```\nowner pet\nAlice dog\n  Bob dog\nAlice cat\n  Bob dog\n```\n::\n::\nSimilarly, we can do the same for JSON format.\nFirst, we prepare a dataset:\n```bash\nprintf \"{\\\"id\\\":1,\\\"owner\\\":\\\"Alice\\\",\\\"pet\\\":\\\"dog\\\"}\n{\\\"id\\\":2,\\\"owner\\\":\\\"Bob\\\",\\\"pet\\\":\\\"dog\\\"}\n{\\\"id\\\":3,\\\"owner\\\":\\\"Bob\\\",\\\"pet\\\":\\\"cat\\\"}\n{\\\"id\\\":4,\\\"owner\\\":\\\"Bob\\\",\\\"pet\\\":\\\"cat\\\"}\" > dataset.jsonlines\n```\nAnd then, we use the method with the \u201cjson\u201d format:\n```python\nt = pw.io.fs.read(\"dataset.jsonlines\", format=\"json\", schema=InputSchema)\n```\nNow let\u2019s try something different. Consider you have site access logs stored in a\nseparate folder in several files. For the sake of simplicity, a log entry contains\nan access ID, an IP address and the login of the user.\nA dataset, corresponding to the format described above can be generated, thanks to the\nfollowing set of unix commands:\n```bash\nmkdir logs\nprintf \"id,ip,login\\n1,127.0.0.1,alice\\n2,8.8.8.8,alice\" > logs/part_1.csv\nprintf \"id,ip,login\\n3,8.8.8.8,bob\\n4,127.0.0.1,alice\" > logs/part_2.csv\n```\nNow, let\u2019s see how you can use the connector in order to read the content of this\ndirectory into a table:\n```python\nclass InputSchema(pw.Schema):\n  ip: str\n  login: str\nt = pw.io.fs.read(\"logs/\", format=\"csv\", schema=InputSchema)\n```\nThe only difference is that you specified the name of the directory instead of the\nfile name, as opposed to what you had done in the previous example. It\u2019s that simple!\nAlternatively, we can do the same for the \u201cjson\u201d variant:\nThe dataset creation would look as follows:\n```bash\nmkdir logs\nprintf \"{\\\"id\\\":1,\\\"ip\\\":\\\"127.0.0.1\\\",\\\"login\\\":\\\"alice\\\"}\n{\\\"id\\\":2,\\\"ip\\\":\\\"8.8.8.8\\\",\\\"login\\\":\\\"alice\\\"}\" > logs/part_1.jsonlines\nprintf \"{\\\"id\\\":3,\\\"ip\\\":\\\"8.8.8.8\\\",\\\"login\\\":\\\"bob\\\"}\n{\\\"id\\\":4,\\\"ip\\\":\\\"127.0.0.1\\\",\\\"login\\\":\\\"alice\\\"}\" > logs/part_2.jsonlines\n```\nWhile reading the data from logs folder can be expressed as:\n```python\nt = pw.io.fs.read(\"logs/\", format=\"json\", schema=InputSchema, mode=\"static\")\n```\nBut what if you are working with a real-time system, which generates logs all the time.\nThe logs are being written and after a while they get into the log directory (this is\nalso called \u201clogs rotation\u201d). Now, consider that there is a need to fetch the new files\nfrom this logs directory all the time. Would Pathway handle that? Sure!\nThe only difference would be in the usage of `mode` field. So the code\nsnippet will look as follows:\n```python\nt = pw.io.fs.read(\"logs/\", format=\"csv\", schema=InputSchema, mode=\"streaming\")\n```\nOr, for the \u201cjson\u201d format case:\n```python\nt = pw.io.fs.read(\"logs/\", format=\"json\", schema=InputSchema, mode=\"streaming\")\n```\nWith this method, you obtain a table updated dynamically. The changes in the logs would incur\nchanges in the Business-Intelligence \u2018BI\u2019-ready data, namely, in the tables you would like to output. To see\nhow these changes are reported by Pathway, have a look at the\n\u201cStreams of Updates and Snapshots\u201d\narticle.\nFinally, a simple example for the plaintext format would look as follows:\n```python\nt = pw.io.fs.read(\"raw_dataset/lines.txt\", format=\"plaintext\")\n```\npw.io.fs.write(table, filename, format)\nWrites `table`\u2019s stream of updates to a file in the given format.\n* Parameters\n    * table (`Table`) \u2013 Table to be written.\n    * filename (`str` | `PathLike`) \u2013 Path to the target output file.\n    * format (`str`) \u2013 Format to use for data output. Currently, there are two supported\n        formats: \u201cjson\u201d and \u201ccsv\u201d.\n* Returns\n    None\nExample:\nIn this simple example you can see how table output works.\nFirst, import Pathway and create a table:\n```python\nimport pathway as pw\nt = pw.debug.table_from_markdown(\"age owner pet \\n 1 10 Alice dog \\n 2 9 Bob cat \\n 3 8 Alice cat\")\n```\nConsider you would want to output the stream of changes of this table in csv format.\nIn order to do that you simply do:\n```python\npw.io.fs.write(t, \"table.csv\", format=\"csv\")\n```\nNow, let\u2019s see what you have on the output:\n```bash\ncat table.csv\n```\n```csv\nage,owner,pet,time,diff\n10,\"Alice\",\"dog\",0,1\n9,\"Bob\",\"cat\",0,1\n8,\"Alice\",\"cat\",0,1\n```\nThe first three columns clearly represent the data columns you have. The column time\nrepresents the number of operations minibatch, in which each of the rows was read. In\nthis example, since the data is static: you have 0. The diff is another\nelement of this stream of updates. In this context, it is 1 because all three rows were read from\nthe input. All in all, the extra information in `time` and `diff` columns - in this case -\nshows us that in the initial minibatch (`time = 0`), you have read three rows and all of\nthem were added to the collection (`diff = 1`).\nAlternatively, this data can be written in JSON format:\n```python\npw.io.fs.write(t, \"table.jsonlines\", format=\"json\")\n```\nThen, we can also check the output file by executing the command:\n```bash\ncat table.jsonlines\n```\n```json\n{\"age\":10,\"owner\":\"Alice\",\"pet\":\"dog\",\"diff\":1,\"time\":0}\n{\"age\":9,\"owner\":\"Bob\",\"pet\":\"cat\",\"diff\":1,\"time\":0}\n{\"age\":8,\"owner\":\"Alice\",\"pet\":\"cat\",\"diff\":1,\"time\":0}\n```\nAs one can easily see, the values remain the same, while the format has changed to a plain JSON.\n"}
{"doc": "Subpackages\n* pathway.stdlib.ml.classifiers package\n    * `knn_lsh_classifier_train()`\n    * `knn_lsh_classify()`\n    * `knn_lsh_euclidean_classifier_train()`\n    * `knn_lsh_generic_classifier_train()`\n    * `knn_lsh_train()`\n    * Submodules\n    * pathway.stdlib.ml.classifiers.test_lsh module\n        * `test_bucketer_cosine()`\n        * `test_bucketer_euclidean()`\n        * `test_lsh()`\n        * `test_lsh_bucketing()`\n* pathway.stdlib.ml.datasets package\n    * Subpackages\n        * pathway.stdlib.ml.datasets.classification package\n* pathway.stdlib.ml.smart_table_ops package\n    * `Edge`\n    * `Feature`\n    * `FuzzyJoinFeatureGeneration`\n        * `FuzzyJoinFeatureGeneration.as_integer_ratio()`\n        * `FuzzyJoinFeatureGeneration.bit_count()`\n        * `FuzzyJoinFeatureGeneration.bit_length()`\n        * `FuzzyJoinFeatureGeneration.conjugate()`\n        * `FuzzyJoinFeatureGeneration.denominator`\n        * `FuzzyJoinFeatureGeneration.from_bytes()`\n        * `FuzzyJoinFeatureGeneration.imag`\n        * `FuzzyJoinFeatureGeneration.numerator`\n        * `FuzzyJoinFeatureGeneration.real`\n        * `FuzzyJoinFeatureGeneration.to_bytes()`\n    * `FuzzyJoinNormalization`\n        * `FuzzyJoinNormalization.as_integer_ratio()`\n        * `FuzzyJoinNormalization.bit_count()`\n        * `FuzzyJoinNormalization.bit_length()`\n        * `FuzzyJoinNormalization.conjugate()`\n        * `FuzzyJoinNormalization.denominator`\n        * `FuzzyJoinNormalization.from_bytes()`\n        * `FuzzyJoinNormalization.imag`\n        * `FuzzyJoinNormalization.numerator`\n        * `FuzzyJoinNormalization.real`\n        * `FuzzyJoinNormalization.to_bytes()`\n    * `JoinResult`\n    * `Node`\n"}