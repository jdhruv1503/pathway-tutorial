{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv pathway[xpack-llm-local] pathway[xpack-llm] openai"
      ],
      "metadata": {
        "id": "x41AY0aQ2j0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRewGcHV1yTp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Microservice for a context-aware ChatGPT assistant.\n",
        "\n",
        "The following program reads in a collection of documents,\n",
        "embeds each document using the OpenAI document embedding model,\n",
        "then builds an index for fast retrieval of documents relevant to a question,\n",
        "effectively replacing a vector database.\n",
        "\n",
        "The program then starts a REST API endpoint serving queries about programming in Pathway.\n",
        "\n",
        "Each query text is first turned into a vector using OpenAI embedding service,\n",
        "then relevant documentation pages are found using a Nearest Neighbor index computed\n",
        "for documents in the corpus. A prompt is built from the relevant documentation pages\n",
        "and sent to the OpenAI chat service for processing.\n",
        "\n",
        "Please check the README.md in this directory for how-to-run instructions.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import dotenv\n",
        "import pathway as pw\n",
        "from pathway.stdlib.ml.index import KNNIndex\n",
        "from pathway.xpacks.llm.embedders import OpenAIEmbedder\n",
        "from pathway.xpacks.llm.llms import OpenAIChat, prompt_chat_single_qa\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "\n",
        "class DocumentInputSchema(pw.Schema):\n",
        "    doc: str\n",
        "\n",
        "\n",
        "class QueryInputSchema(pw.Schema):\n",
        "    query: str\n",
        "    user: str\n",
        "\n",
        "\n",
        "def run(\n",
        "    *,\n",
        "    data_dir: str = os.environ.get(\"PATHWAY_DATA_DIR\", \"_data\"),\n",
        "    api_key: str = os.environ.get(\"OPENAI_API_KEY\", \"\"),\n",
        "    host: str = os.environ.get(\"PATHWAY_REST_CONNECTOR_HOST\", \"0.0.0.0\"),\n",
        "    port: int = int(os.environ.get(\"PATHWAY_REST_CONNECTOR_PORT\", \"8080\")),\n",
        "    embedder_locator: str = \"text-embedding-ada-002\",\n",
        "    embedding_dimension: int = 1536,\n",
        "    model_locator: str = \"gpt-3.5-turbo\",\n",
        "    max_tokens: int = 60,\n",
        "    temperature: float = 0.0,\n",
        "    **kwargs,\n",
        "):\n",
        "    embedder = OpenAIEmbedder(\n",
        "        api_key=api_key,\n",
        "        model=embedder_locator,\n",
        "        retry_strategy=pw.udfs.FixedDelayRetryStrategy(),\n",
        "        cache_strategy=pw.udfs.DefaultCache(),\n",
        "    )\n",
        "\n",
        "    documents = pw.io.jsonlines.read(\n",
        "        data_dir,\n",
        "        schema=DocumentInputSchema,\n",
        "        mode=\"streaming\",\n",
        "        autocommit_duration_ms=50,\n",
        "    )\n",
        "\n",
        "    enriched_documents = documents + documents.select(vector=embedder(pw.this.doc))\n",
        "\n",
        "    index = KNNIndex(\n",
        "        enriched_documents.vector, enriched_documents, n_dimensions=embedding_dimension\n",
        "    )\n",
        "\n",
        "    query, response_writer = pw.io.http.rest_connector(\n",
        "        host=host,\n",
        "        port=port,\n",
        "        schema=QueryInputSchema,\n",
        "        autocommit_duration_ms=50,\n",
        "        delete_completed_queries=True,\n",
        "    )\n",
        "\n",
        "    query += query.select(vector=embedder(pw.this.query))\n",
        "\n",
        "    query_context = query + index.get_nearest_items(\n",
        "        query.vector, k=3, collapse_rows=True\n",
        "    ).select(documents_list=pw.this.doc)\n",
        "\n",
        "    @pw.udf\n",
        "    def build_prompt(documents, query):\n",
        "        docs_str = \"\\n\".join(documents)\n",
        "        prompt = f\"Given the following documents : \\n {docs_str} \\nanswer this query: {query}\"\n",
        "        return prompt\n",
        "\n",
        "    prompt = query_context.select(\n",
        "        prompt=build_prompt(pw.this.documents_list, pw.this.query)\n",
        "    )\n",
        "\n",
        "    model = OpenAIChat(\n",
        "        api_key=api_key,\n",
        "        model=model_locator,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        retry_strategy=pw.udfs.FixedDelayRetryStrategy(),\n",
        "        cache_strategy=pw.udfs.DefaultCache(),\n",
        "    )\n",
        "\n",
        "    responses = prompt.select(\n",
        "        query_id=pw.this.id, result=model(prompt_chat_single_qa(pw.this.prompt))\n",
        "    )\n",
        "\n",
        "    response_writer(responses)\n",
        "\n",
        "    pw.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZEK-G7K2dRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}